{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae68c27b7458864",
   "metadata": {},
   "source": [
    "# Evaluating the performance of ranking model in Search Everywhere\n",
    "\n",
    "In this project, we will evaluate the effectiveness of a ranking models using a sample dataset. \n",
    "We'll start by exploring the differences between the two experiment groups: 0 and 1.\n",
    "Then we'll assess the model performance using two metrics: MRR and Time-To-Click. Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ffd127bb57e14",
   "metadata": {},
   "source": [
    "## 1. Testing the Differences Between Groups\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a268b33b9e45af",
   "metadata": {},
   "source": [
    "### Loading and organizing the data\n",
    "\n",
    "Let's start by loading the necessary libraries and the dataset. Then we'll parse the 'event_data' column from json and create a unique identifier for each (device, session) pair. Then we'll also extract the experiment group from the 'event_data' column and divide the dataset into two groups: 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "id": "a4f74b3ec7168be2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:20.706761Z",
     "start_time": "2024-11-02T10:08:19.588067Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/2024InternshipData.csv')\n",
    "\n",
    "# Parse the event_data JSON column\n",
    "df['event_data'] = df['event_data'].apply(json.loads)\n",
    "\n",
    "# Extract session ID and create a unique identifier for each (device, session) pair\n",
    "df['session_id'] = df['event_data'].apply(lambda x: x['session_id'])\n",
    "df['unique_id'] = df['device_id'] + '_' + df['session_id']\n",
    "\n",
    "# Extract experiment groups into a new column\n",
    "df['experimentGroup'] = df['event_data'].apply(lambda x: x['experimentGroup'])\n",
    "\n",
    "# Split the data into experiment groups\n",
    "groups = {\n",
    "    0: df[df['experimentGroup'] == 0],\n",
    "    1: df[df['experimentGroup'] == 1]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "b8860ab01970868e",
   "metadata": {},
   "source": [
    "### Functions for Analysis\n",
    "Now, let's define few functions to help us with our analysis: \n",
    "* `is_session_successful`: a helper function to check if a session was successful\n",
    "* `calculate_successful_searches`: for calculating the percentage of successful searches (the ones that finished with user choosing a result)\n",
    "* `calculate_average_session_time`: for computing average session durations, with an option to calculate only successful or unsuccessful ones\n",
    "\n",
    "These functions will allow us to compare the interactions of users in each experiment group effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc8c1bbaf68214bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:21.595472Z",
     "start_time": "2024-11-02T10:08:21.586905Z"
    }
   },
   "source": [
    "def is_session_successful(session):\n",
    "    # Check if any of the actions in the session had selectedIndexes (the session was successful)\n",
    "    return any(session['event_data'].apply(lambda x: x['selectedIndexes'] is not None))\n",
    "\n",
    "def calculate_successful_searches(df):\n",
    "    successful_sessions_rows = df.groupby('unique_id').filter(is_session_successful)\n",
    "    successful_searches = successful_sessions_rows['unique_id'].nunique()\n",
    "    finished_searches = len(df[df['event_id'] == 'sessionFinished'])\n",
    "    return successful_searches, successful_searches / finished_searches if finished_searches > 0 else 0\n",
    "\n",
    "def calculate_average_session_time(df, which_ones=\"all\"):\n",
    "    if which_ones == \"all\":\n",
    "        session_durations = df.groupby('unique_id')['time_epoch'].agg(['min', 'max'])\n",
    "    elif which_ones == \"successful\":\n",
    "        successful_sessions = df.groupby('unique_id').filter(is_session_successful)\n",
    "        session_durations = successful_sessions.groupby('unique_id')['time_epoch'].agg(['min', 'max'])\n",
    "    elif which_ones == \"unsuccessful\":\n",
    "        unsuccessful_sessions = df.groupby('unique_id').filter(lambda x: not is_session_successful(x))\n",
    "        session_durations = unsuccessful_sessions.groupby('unique_id')['time_epoch'].agg(['min', 'max'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for 'which_ones'. Use 'all', 'successful', or 'unsuccessful'.\")\n",
    "    \n",
    "    session_durations['duration'] = session_durations['max'] - session_durations['min']\n",
    "    return round(session_durations['duration'].mean() / 1000, 4)  # Convert ms to seconds and round to 4 decimal places"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "989bd9f632af00c3",
   "metadata": {},
   "source": [
    "### Analyzing the Groups \n",
    "Letâ€™s iterate through each group, calculate some statistics, and print them out to compare the two groups. \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9352708463ae8548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:29.898615Z",
     "start_time": "2024-11-02T10:08:22.836095Z"
    }
   },
   "source": [
    "for group_id, group_df in groups.items():\n",
    "    print(f'\\nGroup {group_id} size: {len(group_df)}')\n",
    "\n",
    "    # Successful searches\n",
    "    successful_searches, success_rate = calculate_successful_searches(group_df)\n",
    "    print(f'Group {group_id} successful searches: {successful_searches}')\n",
    "    print(f'Group {group_id} percentage of successful searches: {success_rate:.2%}')\n",
    "\n",
    "    # Average session duration\n",
    "    avg_time_spent = calculate_average_session_time(group_df)\n",
    "    print(f'Group {group_id} average time spent on the Search Everywhere tab: {avg_time_spent}s')\n",
    "    \n",
    "    # Average successful session duration\n",
    "    avg_successful_time_spent = calculate_average_session_time(group_df, which_ones=\"successful\")\n",
    "    print(f'Group {group_id} average time spent on successful searches: {avg_successful_time_spent}s')\n",
    "    \n",
    "    # Average unsuccessful session duration\n",
    "    avg_unsuccessful_time_spent = calculate_average_session_time(group_df, which_ones=\"unsuccessful\")\n",
    "    print(f'Group {group_id} average time spent on unsuccessful searches: {avg_unsuccessful_time_spent}s')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group 0 size: 51012\n",
      "Group 0 successful searches: 4193\n",
      "Group 0 percentage of successful searches: 57.58%\n",
      "Group 0 average time spent on the Search Everywhere tab: 25.7688s\n",
      "Group 0 average time spent on successful searches: 25.6835s\n",
      "Group 0 average time spent on unsuccessful searches: 25.8836s\n",
      "\n",
      "Group 1 size: 56332\n",
      "Group 1 successful searches: 4535\n",
      "Group 1 percentage of successful searches: 56.66%\n",
      "Group 1 average time spent on the Search Everywhere tab: 25.7726s\n",
      "Group 1 average time spent on successful searches: 25.5114s\n",
      "Group 1 average time spent on unsuccessful searches: 26.1109s\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "93da4a846c5d6746",
   "metadata": {},
   "source": [
    "### Observations\n",
    "As we can see, the groups don't seem to have significant differences in terms of sizes, successful searches and average session durations. In fact, regarding the statistics we calculated, the two groups are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3eec65e1a9f752",
   "metadata": {},
   "source": [
    "## 2. Evaluating the Ranking Model\n",
    "---\n",
    "\n",
    "Now, let's evaluate the ranking model using two metrics: Mean Reciprocal Rank (MRR) and Time-To-Click (TTC). We'll calculate these metrics for each group and compare the results. Then we'll calculate the overall MRR and TTC for the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795dc0996c2a6a7",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank (MRR)\n",
    "We'll start by creating a function to calculate the Reciporal Rank for each unique session."
   ]
  },
  {
   "cell_type": "code",
   "id": "bbe2c7e39810bc49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:31.543746Z",
     "start_time": "2024-11-02T10:08:31.539593Z"
    }
   },
   "source": [
    "def calculate_rr(session):\n",
    "    selected_indexes = session.apply(lambda x: x['selectedIndexes'])\n",
    "    \n",
    "    valid_indexes = selected_indexes[selected_indexes.notnull()]\n",
    "    \n",
    "    # Extract the valid list of indexes and the first index from it (the first one that user selected)\n",
    "    return 1 / (valid_indexes.iloc[0][0] + 1) if not valid_indexes.empty else 0"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "d4eaba25349bf416",
   "metadata": {},
   "source": "Now, let's create a function to calculate the Mean Reciprocal Rank (MRR) for a given dataframe and see the results for each group."
  },
  {
   "cell_type": "code",
   "id": "cc0de403f81e5e37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:36.566946Z",
     "start_time": "2024-11-02T10:08:32.485906Z"
    }
   },
   "source": [
    "def calculate_mrr(df):\n",
    "    return df.groupby('unique_id')['event_data'].agg(calculate_rr).mean()\n",
    "    \n",
    "\n",
    "print(f'\\nMRR for Group 0: {calculate_mrr(groups[0]):.4f}')\n",
    "print(f'MRR for Group 1: {calculate_mrr(groups[1]):.4f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MRR for Group 0: 0.1953\n",
      "MRR for Group 1: 0.2164\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "fbdb5cd07b2fa092",
   "metadata": {},
   "source": [
    "As we can see, the model was a bit more gracious for group 1 users and gave them better suggestions (according to the MRR metric). Now let's analyze the model effectiveness using Time-To-Click metric (based on the time between entering the first character and choosing a result)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2ad0c723f9f8c",
   "metadata": {},
   "source": [
    "### Time-To-Click\n",
    "Similarily to MRR, we'll start by creating a function to calculate the time between the first event in the session and the last event in the session. "
   ]
  },
  {
   "cell_type": "code",
   "id": "5307a4f51fd063bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:36.581202Z",
     "start_time": "2024-11-02T10:08:36.576956Z"
    }
   },
   "source": [
    "def calculate_time(session):\n",
    "    time = session['time_epoch']\n",
    "    # Chose the minimum time and subtract it from the maximum time\n",
    "    return time.max() - time.min()"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "5d47717a177fb578",
   "metadata": {},
   "source": [
    "Next, we'll create a function to convert the time to the TTC metric, by applying normalization and\n",
    "then subtracting it from 1 (to get the higher values for smaller times - the faster the user clicked, the better). For unsuccesful searches, we'll return 0."
   ]
  },
  {
   "cell_type": "code",
   "id": "e77afa053a7766ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:39.058930Z",
     "start_time": "2024-11-02T10:08:39.054472Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_normalized_time(session):\n",
    "    return 1 - np.tanh(calculate_time(session)) if is_session_successful(session) else 0"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "28d39d90812291c9",
   "metadata": {},
   "source": [
    "Finally, let's calculate the Time-To-Click metric for each group and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "id": "85a891069d7c456f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:42.476569Z",
     "start_time": "2024-11-02T10:08:39.594434Z"
    }
   },
   "source": [
    "def calculate_ttc(df):\n",
    "    return df.groupby('unique_id').apply(calculate_normalized_time, include_groups=False).mean()\n",
    "\n",
    "print(f'\\nTTC for Group 0: {calculate_ttc(groups[0]):.4f}')\n",
    "print(f'TTC for Group 1: {calculate_ttc(groups[1]):.4f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TTC for Group 0: 0.0680\n",
      "TTC for Group 1: 0.0688\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "b9633f3e7cde2cd7",
   "metadata": {},
   "source": "As we can see, the model performed better for group 1 users again. Or the group 0 users just clicked on the results slower..."
  },
  {
   "cell_type": "markdown",
   "id": "1102dc069f90cb50",
   "metadata": {},
   "source": [
    "### Last but not least\n",
    "At last, lets calculate the overall MRR and TTC for the entire dataset and combine them into a single metric, which\n",
    "I'll call RAT (Rank And Time) - the mean of MRR and TTC."
   ]
  },
  {
   "cell_type": "code",
   "id": "965c103b2dbba993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:52.541767Z",
     "start_time": "2024-11-02T10:08:45.731795Z"
    }
   },
   "source": [
    "mrr = calculate_mrr(df)\n",
    "print(f'\\nOverall MRR: {mrr:.4f}')\n",
    "ttc = calculate_ttc(df)\n",
    "print(f'Overall TTC: {ttc:.4f}')\n",
    "rat = (mrr + ttc) / 2\n",
    "print(f'Overall RAT: {rat:.4f}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall MRR: 0.2063\n",
      "Overall TTC: 0.0684\n",
      "Overall RAT: 0.1374\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "2ebad92d3745683d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We started this project with analyzing the differences between to experiment groups, gathered some statistics and found out that the groups are almost identical in terms of their interactions with the Search Everywhere tab. Then, we evaluated the ranking model using two metrics: Mean Reciprocal Rank (MRR) and Time-To-Click (TTC). The model seemed to perform slightly better for group 1 users in terms of both MRR and TTC metrics. Finally, we calculated the overall MRR and TTC for the entire dataset and combined them into a single metric called RAT. The RAT value gives us a good overview of the model's performance in terms of both ranking and time-to-click. It doesn't really tell us much about the model's performance itself, but it gives us a way to compare different models or model versions. "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T10:08:09.309889Z",
     "start_time": "2024-11-02T10:08:09.306914Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "efb4fe32cd57a1bb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
